# -*- coding: utf-8 -*-
"""MNCH PROJECT CODES

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13DZPSikcGUfTfEgteZNA7pI85Pv9Xl1D

# MACHINE LEARNING

## Loading in the dataset
"""

import pandas as pd

df = pd.read_csv("/content/drive/MyDrive/project1.csv")

df.head()

"""# Importing libraries"""

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import missingno as msno
import datetime as dt
import pandas as pd
from sklearn import tree
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestRegressor
# Modelling
from imblearn.over_sampling import SMOTE
from sklearn.metrics import accuracy_score, roc_auc_score
from sklearn.model_selection import RepeatedStratifiedKFold, cross_validate
from sklearn.multiclass import (
    OneVsOneClassifier,
    OneVsRestClassifier,
    OutputCodeClassifier,
)

import xgboost as xgb
from xgboost import XGBClassifier
from imblearn.under_sampling import RandomUnderSampler
from imblearn.over_sampling import RandomOverSampler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay
from sklearn.model_selection import RandomizedSearchCV, train_test_split
from scipy.stats import randint
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay
from sklearn.model_selection import RandomizedSearchCV
import warnings as wr

wr.filterwarnings('ignore')
print('Libraries Imported')

"""## Check for missing values and count them"""

missing_values = df.isnull().sum()
missing_values

"""## Converting objects to float"""

df['ANC'] = pd.to_numeric(df['ANC'], errors='coerce')

df['PNC'] = pd.to_numeric(df['PNC'], errors='coerce')

df['SBA'] = pd.to_numeric(df['SBA'], errors='coerce')

df['SBA'] = pd.to_numeric(df['SBA'], errors='coerce')

df['Distance to health'] = pd.to_numeric(df['Distance to health'], errors='coerce')

"""# Getting Counts of Variables"""

variable_counts = df['wealth index'].value_counts()
print(variable_counts)

variable_counts = df['Head age'].value_counts()
print(variable_counts)

variable_counts = df['Freq. of watching TV'].value_counts()
print(variable_counts)

variable_counts = df['Religion'].value_counts()
print(variable_counts)

variable_counts = df['Birth order'].value_counts()
print(variable_counts)

variable_counts = df['Distance to health'].value_counts()
print(variable_counts)

variable_counts = df['Current marital_status'].value_counts()
print(variable_counts)

variable_counts = df['Freq. of watching TV'].value_counts()
print(variable_counts)

variable_counts = df['Freq. of listening to radio'].value_counts()
print(variable_counts)

"""# Handling Missing Values"""

# Replace non-numeric values with NaN
df_numeric = df.apply(pd.to_numeric, errors='coerce')

old_number = 96
new_number = 11

df['Religion'] = df['Religion'].replace(old_number, new_number)

df.columns

df_filled = df.drop(['Household wealth', 'Birth order'], axis=1)

df_filled['Distance to health'] = df_filled['Distance to health'].fillna(df_filled['Distance to health'].median())

# Fill NaN cells with the median of each column
df_filled.fillna(df_filled.median())

"""# Exploratory Data Analysis (EDA)

"""

df_filled.describe()

cat_cols=df[['Education level','Region','Place of Residence','Current marital_status','wealth index','SBA','Head age']]

for col in cat_cols:
    plt.figure(figsize=[10,5])
    sns.countplot(df,x=df[col]).set(title= col+' Value Distribution')
    plt.show()

import pandas as pd

# Assuming new_df contains your DataFrame
# Compute the correlation matrix
correlation_matrix = df_filled.corr()

import seaborn as sns
import matplotlib.pyplot as plt

# Plotting the correlation matrix as a heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of Variables')
plt.show()

# Set Seaborn style
sns.set_style("darkgrid")

# Identify numerical columns
numerical_columns = df.select_dtypes(include=["int64", "float64"]).columns

# Plot distribution of each numerical feature
plt.figure(figsize=(14, len(numerical_columns) * 3))
for idx, feature in enumerate(numerical_columns, 1):
    plt.subplot(len(numerical_columns), 2, idx)
    sns.histplot(df[feature], kde=False)
    plt.title(f"{feature} | Skewness: {round(df[feature].skew(), 2)}")

# Adjust layout and show plots
plt.tight_layout()
plt.show()

"""# Extracting Features Variables and Target Variable

"""

X = df_filled.iloc[:, :-3].values
y = df_filled.iloc[:, -3:].values

# Creating a new DataFrame with column names
columns_to_keep = df_filled.columns[:-3]  # Exclude the last three columns
new_data_X = pd.DataFrame(X, columns=columns_to_keep)

print(new_data_X.info())

print(new_data_X.dtypes)

column_names_y = df_filled.columns[-3:]  # Assuming you want to use the original column names
new_data_y = pd.DataFrame(y, columns=column_names_y)

print(new_data_y.dtypes)

new_data_y['dropout combined']=0

new_data_y.loc[new_data_y['ANC'] < 4, 'dropout combined'] = 1
new_data_y.loc[new_data_y['SBA'] == 0, 'dropout combined'] = 2
new_data_y.loc[new_data_y['PNC'] == 0, 'dropout combined'] = 3

new_data_y.drop(['ANC','SBA','PNC'], axis=1, inplace=True)

"""## Visualizing Counts in Target variable"""

from matplotlib import pyplot as plt
new_data_y['dropout combined'].plot(kind='hist', bins=20, title='dropout combined')
plt.gca().spines[['top', 'right',]].set_visible(True)

import matplotlib.pyplot as plt
import seaborn as sns

filtered_data = new_data_y[new_data_y['dropout combined'] != 0]
sns.countplot(data=filtered_data, x='dropout combined')
plt.title('Count of Dropout Events')
plt.xlabel('Dropout Classes')
plt.ylabel('Count')
plt.show()

"""## Handling the Data."""

# class count
class_count_1, class_count_2, class_count_3, class_count_4= new_data_y['dropout combined'].value_counts()

# Separate class
class_0 = new_data_y[new_data_y['dropout combined'] == 0]
class_1 = new_data_y[new_data_y['dropout combined'] == 1]
class_2 = new_data_y[new_data_y['dropout combined'] == 2]
class_3= new_data_y[new_data_y['dropout combined'] == 3]

# print the shape of the class
print('class 0:', class_0.shape)
print('class 1:', class_1.shape)
print('class 2:', class_2.shape)
print('class 3:', class_3.shape)

from collections import Counter
from matplotlib import pyplot
from sklearn.preprocessing import LabelEncoder

# label encode the target variable
# This code prepares the target variable (new_data_y) for machine learning models that require numerical inputs
y = LabelEncoder().fit_transform(new_data_y)

X = new_data_X
# transform the dataset

# transform the dataset
strategy = {0:25318, 1:40000, 2:25318, 3:40000}
oversample = SMOTE(sampling_strategy=strategy)
X, y = oversample.fit_resample(X, y)
# summarize distribution
counter = Counter(y)
for k,v in counter.items():
    per = v / len(y) * 100
    print('Class=%d, n=%d (%.3f%%)' % (k, v, per))

# plot the distribution
pyplot.bar(counter.keys(), counter.values())
pyplot.show()

"""# Resampled Data Analysis

# Splitting the dataset into Training and Testing sets
"""

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Apply SMOTE to handle class imbalance
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

"""# *Random Forest*"""

# Train a Random Forest classifier on the resampled data
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train_resampled, y_train_resampled)

# Make predictions on the test set
rf_predictions = rf_model.predict(X_test)

# Evaluate the model
rf_accuracy = accuracy_score(y_test, rf_predictions)
print("Random Forest Accuracy on Test Set:", rf_accuracy)

# Additional Evaluation Metrics
print("Classification Report:")
print(classification_report(y_test, rf_predictions))
print("Confusion Matrix:")
print(confusion_matrix(y_test, rf_predictions))

# Feature Importance from the trained Random Forest model
feature_importance = pd.DataFrame({'Feature': X.columns, 'Importance': rf_model.feature_importances_})
feature_importance = feature_importance.sort_values(by='Importance', ascending=False)
print("Feature Importance:")
print(feature_importance)

"""# *XGBoost*

Training and Testing
"""

# Train an XGBoost classifier on the resampled data
xgb_model = XGBClassifier(n_estimators=190, min_child_weight=4, subsample= 0.7999999999999999, max_depth=19, learning_rate=0.1, random_state=42)
xgb_model.fit(X_train_resampled, y_train_resampled)

# Make predictions on the test set
xgb_predictions = xgb_model.predict(X_test)

# Evaluate the model
xgb_accuracy = accuracy_score(y_test, xgb_predictions)
print("XGBoost Accuracy on Test Set:", xgb_accuracy)

# Get feature importances
xgb_feature_importances = xgb_model.feature_importances_

# Sort feature importances in ascending order
sorted_indices = np.argsort(xgb_feature_importances)
sorted_feature_importances = xgb_feature_importances[sorted_indices]
sorted_columns = X_train_resampled.columns[sorted_indices]

# Plotting feature importance for XGBoost
plt.figure(figsize=(10, 6))
plt.barh(range(len(sorted_feature_importances)), sorted_feature_importances, color='blue')
# Highlight top 4 features in red
for i in range(1, 5):
    plt.barh(len(sorted_feature_importances) - i, sorted_feature_importances[-i], color='red')

# Set y-axis labels to feature names
plt.yticks(range(len(sorted_columns)), sorted_columns)
plt.xlabel('Feature Importance')
plt.ylabel('Features')
plt.title('XGBoost Feature Importance')
plt.show()

# Additional Evaluation Metrics
print("Classification Report:")
print(classification_report(y_test, xgb_predictions))
print("Confusion Matrix:")
print(confusion_matrix(y_test, xgb_predictions))

# Feature Importance from the trained XGBoost model
feature_importance = pd.DataFrame({'Feature': X.columns, 'Importance': xgb_model.feature_importances_})
feature_importance = feature_importance.sort_values(by='Importance', ascending=False)
print("Feature Importance:")
print(feature_importance)

"""

# Determining Best HyperParameters of XGBoost Model.

"""

param_grid = {
    'n_estimators': np.arange(10, 300),
    'learning_rate': np.arange(0.1,0.2),
    'max_depth': np.arange(1, 20),
    'min_child_weight': np.arange(1, 10, 1),
    'subsample': np.arange(0.6, 1.0, 0.1),
    'colsample_bytree': np.arange(0.6, 1.0, 0.1),
    'bootstrap': [True, False]
}

xgb_model = XGBClassifier()

random_search = RandomizedSearchCV(
    xgb_model, param_distributions=param_grid, n_iter=20,
    scoring='neg_mean_squared_error', cv=3, verbose=1, n_jobs=-1, random_state=42
)

random_search.fit(X_train, y_train)

print("Best Hyperparameters:", random_search.best_params_)

# Generate the confusion matrix
conf_matrix = confusion_matrix(y_test, xgb_predictions)

# Print or visualize the confusion matrix
print("Confusion Matrix:")
print(conf_matrix)

"""# Feature Importance

## *XGBoost Feature Importance*
"""

# Get feature importances
importances = xgb_model.feature_importances_
feature_names = X.columns

# Create a DataFrame with feature names and their importances
feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})

# Sort the DataFrame by importance in descending order
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=True)

# Plot the top N feature importances
top_n = 15  # You can adjust the number of features to display
plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['Feature'][:top_n], feature_importance_df['Importance'][:top_n], color='skyblue')
plt.xlabel('Feature Importance')
plt.title('XGBoost Feature Importances')
plt.show()

"""## *Random Forest Feature Importance*"""

# Get feature importances
importances1 = rf_model.feature_importances_
feature_names = X.columns

# Create a DataFrame with feature names and their importances
feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances1})

# Sort the DataFrame by importance in descending order
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=True)

# Plot the top N feature importances
top_n = 15  # You can adjust the number of features to display
plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['Feature'][:top_n], feature_importance_df['Importance'][:top_n], color='skyblue')
plt.xlabel('Feature Importance')
plt.title('Random Forest Feature Importances')
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Assuming you have imported necessary data and defined variables like X, importances1, and importances2

# Extracting features and importances
features = X.columns
importances_model1 = list(importances1.tolist())
importances_model2 = list(importances.tolist())

# Check if lengths of feature lists are the same
if len(features) != len(importances_model1) or len(features) != len(importances_model2):
    raise ValueError("Lengths of feature lists and importances must be the same")

# Sort features and importances by importances
sorted_indices_model1 = np.argsort(importances_model1)
sorted_indices_model2 = np.argsort(importances_model2)
sorted_features_model1 = [features[i] for i in sorted_indices_model1]
sorted_features_model2 = [features[i] for i in sorted_indices_model2]
sorted_importances_model1 = [importances_model1[i] for i in sorted_indices_model1]
sorted_importances_model2 = [importances_model2[i] for i in sorted_indices_model2]

# Set width of bar
bar_width = 0.35

# Set positions of bars on y-axis
r1 = np.arange(len(features))
r2 = [y + bar_width for y in r1]

# Create bar plot
plt.barh(r1, sorted_importances_model1, color='b', height=bar_width, edgecolor='grey', label='Random Forest')
plt.barh(r2, sorted_importances_model2, color='r', height=bar_width, edgecolor='grey', label='XGBoost')

# Add yticks on the middle of the group bars
plt.ylabel('Features', fontweight='bold')
plt.yticks([r + bar_width/2 for r in range(len(features))], sorted_features_model2)

# Add xlabel
plt.xlabel('Importance', fontweight='bold')

# Add legend
plt.legend()

# Show plot
plt.tight_layout()  # Adjust layout to prevent clipping of labels
plt.show()

"""# Performance of the Model

## *Confusion Matrix*
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from xgboost import XGBClassifier
import seaborn as sns
import matplotlib.pyplot as plt


# Generate the confusion matrix
conf_matrix = confusion_matrix(y_test, xgb_predictions)

# Plot an attractive confusion matrix
plt.figure(figsize=(10, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=np.unique(y), yticklabels=np.unique(y))
plt.title('XGBoost Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

# Generate the confusion matrix
conf_matrix = confusion_matrix(y_test, rf_predictions)

# Plot an attractive confusion matrix
plt.figure(figsize=(10, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=np.unique(y), yticklabels=np.unique(y))
plt.title('Random Forest Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""
# Downloading the Model"""

from xgboost import XGBClassifier
import joblib

# Save the model to a file
joblib.dump(xgb_model, 'model.pkl')


# Download the saved model file
from google.colab import files
files.download('model.pkl')

import pickle
pickle.dump(xgb_model, open('/content/model.pkl', 'wb'))

import pickle
pickle.dump(xgb_model, open('./model.sav', 'wb'))

"""# Geo- Mapping the Predictions"""

!pip install geopandas folium

import geopandas as gpd
import pandas as pd

# Load GeoJSON data for Kenyan counties
geojson_path = '/content/drive/MyDrive/kenya-counties-simplified.geojson'
kenya_geojson = gpd.read_file(geojson_path)

mnch_dropout_data = pd.DataFrame({
    'REGION': kenya_geojson['shapeName'],
    'Dropout_Rate': new_data_y['dropout combined']
})

kenya_geojson['shapeName'] = kenya_geojson['shapeName'].astype(str)
mnch_dropout_data['REGION'] = mnch_dropout_data['REGION'].astype(str)

merged_data = kenya_geojson.merge(mnch_dropout_data, left_on='shapeName', right_on='REGION')
merged_data

import folium

# Create a map centered around Kenya
kenya_map = folium.Map(location=[1.2921, 36.8219], zoom_start=6)

# Add choropleth layer
folium.Choropleth(
    geo_data=merged_data,
    name='choropleth',
    data=merged_data,
    columns=['REGION', 'Dropout_Rate'],
    key_on='feature.properties.shapeName',
    fill_color='YlOrRd',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='MNCH Dropout Rates (%)'
).add_to(kenya_map)

# Add layer control
folium.LayerControl().add_to(kenya_map)

# Show the map
kenya_map

mnch_dropout_data1 = pd.DataFrame({
    'REGION': kenya_geojson['shapeName'],
    'Current marital_statusR': new_data_X['Current marital_status']
})

kenya_geojson['shapeName'] = kenya_geojson['shapeName'].astype(str)
mnch_dropout_data1['REGION'] = mnch_dropout_data1['REGION'].astype(str)

merged_data1 = kenya_geojson.merge(mnch_dropout_data1, left_on='shapeName', right_on='REGION', how='inner')

print(merged_data1.columns)

mnch_dropout_data = pd.DataFrame({
    'REGION': kenya_geojson['shapeName'],
    'CurrAgeGroup': new_data_X[' CurrAgeGroup']
})

kenya_geojson['shapeName'] = kenya_geojson['shapeName'].astype(str)
mnch_dropout_data['REGION'] = mnch_dropout_data['REGION'].astype(str)

merged_data = kenya_geojson.merge(mnch_dropout_data, left_on='shapeName', right_on='REGION')

!pip install geopandas
import folium
import geopandas as gpd

# Create a map centered around Kenya
kenya_map = folium.Map(location=[1.2921, 36.8219], zoom_start=6)

# Add choropleth layer
folium.Choropleth(
    geo_data=merged_data,
    name='choropleth',
    data=merged_data,
    columns=['REGION', 'CurrAgeGroup'],
    key_on='feature.properties.shapeName',
    fill_color='YlOrRd',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Current Age Groups'
).add_to(kenya_map)

# Add layer control
folium.LayerControl().add_to(kenya_map)

# Show the map
kenya_map
